* basic-webcrawler

The goal of this mini project is to learn the basics of multi-threading.

* Features

- Respects robots.txt file.
- Use delays between requests to not be annoying. Default value of 1 second between requests, but prefer delay specified in robots.txt.
- Multi-threaded.
- Configurable parameters in `CMakeLists`:
  - SEED_URL_1: specifies initial URL to start crawling.
  - DEFAULT_REQUEST_DELAY: specifies the delay in seconds between requests to the same domain.
  - DEFAULT_DEPTH_LIMIT: specifies the depth of the current search.
  - METADATA_FILENAME: specifies the name of the file where the program will dump its output on.
- Avoids doing requests to useless links like pdfs, jpgs, pngs, login/auth pages, embedded javascript, etc.

* Dependencies

- cmake
- c++ compiler with c++17 support
- vcpkg
- lexbor library (for http parsing)
- curl library (for http requests)

I still don't know how to use vcpkg effectively so I can't provide a good tutorial on how to do install dependencies using it, but the way I did it was:

1. Create a directory (parent of this project) called vcpkg.
2. Install vcpkg there.
3. Use vcpkg to manually install lexbor and curl.
4. Then, in `compile.sh`, use this flag: `-DCMAKE_TOOLCHAIN_FILE=../vcpkg/scripts/buildsystems/vcpkg.cmake`.
5. Notice that inside `CMakeLists.txt` there are also references to this directory. This is not ideal, but lexbor didn't provide a cmake integration so couldn't do anything about it.

* Notes

There's a lot of room for improvement. This is just a very basic example of me learning the basics of multi-threading.

I didn't consider serious stuff like load-balancing, lock-free data structures, and so on. There's probably a lot of contention.

Also, since the focus of this project was multi-threading, I didn't provide enough and serious tests (besides a basic one to know if I'm parsing links correctly). Ideally I'd want them. It'd be nice to also use a lib like GTest for more expressive tests with cmake integration.

* Compilation

Run `compile.sh` and then `./build/basic-webcrawler`.

You'll get messages in stderr if there are any errors like timeouts to certain websites, couldn't access website, etc.

For every successful website processed, you'll have an entry on `METADATA_FILENAME`.
